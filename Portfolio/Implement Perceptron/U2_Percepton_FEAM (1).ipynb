{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron Implementation**"
      ],
      "metadata": {
        "id": "SBcekIOMNIKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **Explanation of the intuition behind the Perceptron algorithm:**\n",
        "\n",
        "The Perceptron is a supervised learning algorithm for binary classification, developed by Frank Rosenblatt in 1957. The intuition behind the Perceptron is based on replicating the functioning of a neuron in the human brain. The intuition behind the Perceptron algorithm is explained below:\n",
        "\n",
        "1. Artificial Neuron:\n",
        "A Perceptron can be seen as an artificial neuron. Like neurons in the human brain, it takes various inputs, processes them, and produces an output.\n",
        "\n",
        "2. Tickets and Weights:\n",
        "Entries (x): Each entry represents a characteristic of the data being classified.\n",
        "Weights (w): Each entry has an associated weight that indicates its relative importance in the classification decision. The weights are parameters of the model and are adjusted during training.\n",
        "3. Weighted Sum:\n",
        "The entries are multiplied by their respective weights and added to form a single quantity\n",
        "\n",
        "4. Activation Function:\n",
        "The weighted sum is passed through an activation function. The activation function in the Perceptron is a step function (also known as Heaviside function) that converts the weighted sum into a binary output (0 or 1). The output indicates the class to which the data is assigned.\n",
        "\n",
        "5. Learning:\n",
        "During training, the Perceptron adjusts its weights to minimize classification error. If the classification is incorrect, the weights are updated to reduce the error in the next prediction. This process is repeated for several training examples until the model converges to a solution that correctly classifies the training data.\n",
        "\n",
        "6. Limitations:\n",
        "The Perceptron has important limitations, such as the inability to learn complex patterns and the limitation to linearly separable problems. However, these limitations were addressed by later models, such as multilayer neural networks, which allow classification of nonlinearly separable data.\n"
      ],
      "metadata": {
        "id": "FEtZgT53N7wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   **Algorithm pseudocode:**\n",
        "\n",
        "Perceptron function (training, classes, learning_rate, number_of_iterations):\n",
        "     Randomly initialize weights for each feature (w)\n",
        "     Initialize threshold (b) randomly or to 0\n",
        "    \n",
        "     For each iteration from 1 to number_of_iterations:\n",
        "         For each example, class in zip(training, classes):\n",
        "             Calculate the weighted sum:\n",
        "             weighted_sum = 0\n",
        "             For each feature, value in zip(training[example], w):\n",
        "                 weighted_sum += value * characteristic\n",
        "             weighted_sum += b\n",
        "            \n",
        "             Apply the step activation function:\n",
        "             If weighted_sum > 0, then prediction = 1, else prediction = 0\n",
        "            \n",
        "             Update weights and threshold if prediction is incorrect:\n",
        "             If prediction != class:\n",
        "                 For each feature, value in zip(training[example], w):\n",
        "                     w = w + learning_rate * (class - prediction) * value\n",
        "                 b = b + learning_rate * (class - prediction)\n",
        "    \n",
        "     Return weights (w) and threshold (b)"
      ],
      "metadata": {
        "id": "Y10ZchUPOcCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   **Algorithm implementation in Python:**"
      ],
      "metadata": {
        "id": "-U8WLfqNPaBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The NumPy library is imported for matrix manipulation and mathematical operations.\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "47BrN7_hKYMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A class called Perceptron is defined. In the __init__ method, the attributes of the Perceptron are initialized.\n",
        "#Including the number of features (num_features), the learning rate (learning_rate), the number of training iterations (num_iterations), the weights (weights), and the bias (bias).\n",
        "class Perceptron:\n",
        "    def __init__(self, num_features, learning_rate=0.01, num_iterations=1000):\n",
        "        self.num_features = num_features\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "#The predict method takes a set of features and returns the Perceptron prediction.\n",
        "#Calculates the weighted sum of the features and weights (np.dot(features, self.weights) + self.bias) and applies the step function to decide whether the output is 1 or 0.\n",
        "    def predict(self, features):\n",
        "        return 1 if np.dot(features, self.weights) + self.bias > 0 else 0\n",
        "\n",
        "#The train method performs the training of the Perceptron.\n",
        "#Iterates over the specified number of iterations (num_iterations) and for each iteration, iterates over the training data (training_data) and the corresponding labels (labels).\n",
        "#It calculates the Perceptron prediction, compares with the actual label, and adjusts the weights and bias accordingly to correct errors.\n",
        "    def train(self, training_data, labels):\n",
        "        for _ in range(self.num_iterations):\n",
        "            for features, label in zip(training_data, labels):\n",
        "                prediction = self.predict(features)\n",
        "                self.weights += self.learning_rate * (label - prediction) * features\n",
        "                self.bias += self.learning_rate * (label - prediction)"
      ],
      "metadata": {
        "id": "-cjyOHVnKbTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An instance of the Perceptron class is created with 2 features (for the 2 inputs of the AND logic gate). The Perceptron is then trained with the training data and tested with some data to see the predictions."
      ],
      "metadata": {
        "id": "-C2IHdYiRz4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data (AND gate)\n",
        "training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create a Perceptron with 2 characteristics (for the 2 inputs)\n",
        "perceptron = Perceptron(num_features=2)\n",
        "\n",
        "# Train the Perceptron\n",
        "perceptron.train(training_data, labels)\n",
        "\n",
        "# Test the trained Perceptron\n",
        "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for features in test_data:\n",
        "     prediction = perceptron.predict(features)\n",
        "     print(f'Inputs: {features} -> Prediction: {prediction}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aKNT-IsKt6y",
        "outputId": "2cc5d02b-f248-4fe6-abae-3e79f3b442c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [0 0] -> Prediction: 0\n",
            "Inputs: [0 1] -> Prediction: 0\n",
            "Inputs: [1 0] -> Prediction: 0\n",
            "Inputs: [1 1] -> Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   **Loss function + Optimization function identification.:**\n",
        "\n",
        "No, the Perceptron does not use a loss function or optimization algorithm like traditional supervised learning methods. Instead, it uses a simple, bug-fix-based weight update rule. Weight updating occurs only when a sample is incorrectly classified. The step activation function is used to make classification decisions (0 or 1) based on the weighted sum of the inputs and weights. If the classification is incorrect, the weights are updated to correct the error.\n",
        "\n",
        "Since the Perceptron can only learn linear classifications (data that can be divided into two classes with a single straight line), a continuous loss function is not needed. The weight update rule guarantees that the Perceptron converges to a solution if the data are linearly separable; that is, if it is possible to draw a straight line that divides the two classes."
      ],
      "metadata": {
        "id": "87ZTx06BQyx0"
      }
    }
  ]
}