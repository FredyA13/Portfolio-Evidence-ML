{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.   **Explanation of the intuition behind the KNN algorithm.**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple but effective supervised learning algorithm used for classification and regression. The intuition behind the KNN algorithm is quite simple:\n",
        "\n",
        "\n",
        "1.   Near Neighbors: In essence, KNN makes assumptions based on similarity: if an object is similar to other known objects, it is likely to belong to the same category or have a similar value. Imagine that you have points in an n-dimensional space (where n is the number of features or attributes in the data set). KNN is based on the idea that nearby points in this multidimensional space have similar labels or values.\n",
        "\n",
        "2.   K Value: “K” in KNN represents the number of nearest neighbors that will be taken into account when making a prediction. For example, if K=3, the algorithm will find the 3 points closest to the point you are trying to classify or predict.\n",
        "\n",
        "\n",
        "3.   Distance Function: To determine which points are closest, KNN uses a distance function, commonly the Euclidean distance in Euclidean space. The Euclidean distance between two points. On a 2D plane is calculated as:\n",
        "Distance = <img src=\"/content/WhatsApp Image 2023-10-09 at 9.40.55 PM.jpeg\" alt=\"Distance\">\n",
        "\n",
        "4.   Classification and Regression: In the case of classification, once the K nearest neighbors of a test point have been identified, the test point is classified into the category that is most common among its K nearest neighbors. In the case of regression, the average of the values of the K nearest neighbors is taken as the prediction value for the test point.\n",
        "\n",
        "\n",
        "5.   Non-Parametric and Lazy Learning: KNN is a non-parametric algorithm, meaning it makes no assumptions about the shape of the underlying data. Additionally, it is an example of \"lazy learning\", meaning that it does not explicitly learn a model during the training phase. Instead, it stores all the training data and performs calculations at prediction time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dm4yIyKFpiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2.   **Algorithm pseudocode:**\n",
        "\n",
        "\n",
        "\n",
        "*   For Classification:\n",
        "\n",
        "1.   For each point in the data set:\n",
        "\n",
        "     Calculate the distance between the test point and the data set point.\n",
        "\n",
        "     Stores the distance and label/class of the point.\n",
        "\n",
        "2.   Sort the distances in ascending order.\n",
        "3.   Take the first K labels of the closest points.\n",
        "4.   Count the occurrences of each label within the K nearby points.\n",
        "5.   The tag/class with the most occurrences is the prediction for the test     point.\n",
        "\n",
        "\n",
        "*   For Regression:\n",
        "\n",
        "1.   For each point in the data set:\n",
        "\n",
        "     Calculate the distance between the test point and the data set point.\n",
        "\n",
        "     Stores the distance and value of the point.\n",
        "\n",
        "2.   Sort the distances in ascending order.\n",
        "3.   Take the first K values of the closest points.\n",
        "4.   Calculate the average of these values.\n",
        "5.   The average is the prediction for the test point.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NNNDAEAOs_qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   **Implementation of the algorithm (own) in Python (Jupyter Notebooks preferably).:**"
      ],
      "metadata": {
        "id": "ATPAG8r8xU1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk7J6_XEQOiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c97faa1-8cf0-44e8-d173-fcb951493af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of training set X: (120, 4)\n",
            "Dimensions of test set X: (30, 4)\n",
            "Dimensions of training set y: (120,)\n",
            "Dimensions of test set y: (30,)\n",
            "KNN model accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Get features (X) and labels (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "#Splits the data set into training and test sets. 80% of the data is used to train the model (X_train and y_train), while 20% is used to test the model (X_test and y_test).\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# View the dimensions of the training and test sets\n",
        "print(\"Dimensions of training set X:\", X_train.shape)\n",
        "print(\"Dimensions of test set X:\", X_test.shape)\n",
        "print(\"Dimensions of training set y:\", y_train.shape)\n",
        "print(\"Dimensions of test set y:\", y_test.shape)\n",
        "\n",
        "# Create a KNN classifier with K=3\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the classifier with the training data\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"KNN model accuracy: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 100% accuracy on the test set indicates that the model has successfully learned to classify the flowers into the correct categories.\n",
        "\n",
        "Obtaining 100% accuracy on a data set may indicate that the model is overfitting the training data. This means that the model has learned too specifically the training data and might not generalize well to new data that it has not seen before. To avoid overfitting, it is important to consider techniques such as cross-validation and hyperparameter optimization to evaluate and tune the model more accurately."
      ],
      "metadata": {
        "id": "3HFe4hiF1MmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   **Loss function + Optimization function identification:**\n",
        "\n",
        "K-Nearest Neighbors (KNN) does not have an associated loss function or optimization function like other supervised learning algorithms such as neural networks or gradient-based algorithms such as gradient descent.\n",
        "\n",
        "KNN is a lazy learning algorithm, which means that it does not learn a model during the training phase. Instead of using a loss function to optimize parameters, KNN simply stores the training data and, during the testing phase, compares the similarity between the new point and the training points to make predictions based on the majority of votes (in the case of classification) or on the average (in the case of regression) of the nearest neighbors."
      ],
      "metadata": {
        "id": "IFkXp3E8xuSq"
      }
    }
  ]
}